{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>243</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB NAME = Mayur:New_Policy_Addition\n",
      "No_of_cpu=2\n",
      "Max_cores=2\n",
      "Executor_mem=10g"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# runtype = \"daily\"\n",
    "# report_date = \"2021-02-28\" #automated\n",
    "\n",
    "no_of_cpu = 2\n",
    "max_cores = 2\n",
    "executor_mem = '10g'\n",
    "\n",
    "\n",
    "Job_Name = 'Mayur:New_Policy_Addition'\n",
    "\n",
    "\n",
    "# print \"JOB NAME = \"+str(Job_Name)\n",
    "# print \"No_of_cpu=\"+str(no_of_cpu)\n",
    "# print \"Max_cores=\"+str(max_cores)\n",
    "# print \"Executor_mem=\"+str(executor_mem)\n",
    "# print \"Runtype=\"+str(runtype)\n",
    "# print \"Report_date=\"+str(report_date)\n",
    "\n",
    "\n",
    "print (\"JOB NAME = \"+str(Job_Name))\n",
    "print (\"No_of_cpu=\"+str(no_of_cpu))\n",
    "print (\"Max_cores=\"+str(max_cores))\n",
    "print (\"Executor_mem=\"+str(executor_mem))\n",
    "# print (\"Runtype=\"+str(runtype))\n",
    "# print (\"Report_date=\"+str(report_date))\n",
    "\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import asc,lit\n",
    "#warnings.filterwarnings('error')\n",
    "import pyspark\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "#import numpy\n",
    "import calendar\n",
    "#import pandas as pd\n",
    "#import simplejson as json\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import lit\n",
    "import simplejson as json\n",
    "import json, pprint, requests\n",
    "#es_nodes = '10.35.12.5'\n",
    "#es_nodes = '10.35.12.6'\n",
    "# es_nodes = '10.35.12.194'#,10.35.12.6,10.35.12.5\n",
    "# es_nodes_temp='10.35.12.194'\n",
    "# es_port = '5432'\n",
    "# es_user = 'gpanalytics'\n",
    "# es_pwd = ''\n",
    "mesos_ip = 'mesos://10.33.195.18:5050'#'mesos://10.35.12.5:5050'\n",
    "spark.stop() #############NEED TO COMMENT THIS SPARK.STOP WHEN RUNNING THROUGH SHELL###############################\n",
    "conf.setMaster(mesos_ip)\n",
    "\n",
    "conf.set('spark.executor.cores',no_of_cpu) ### change 1\n",
    "#conf.set('spark.memory.fraction','.2')\n",
    "conf.set('spark.executor.memory',executor_mem) \n",
    "conf.set('spark.es.scroll.size','10000')\n",
    "conf.set('spark.network.timeout','600s')\n",
    "conf.set('spark.sql.crossJoin.enabled', 'true')\n",
    "conf.set('spark.sql.join.preferSortMergeJoin','true')\n",
    "\n",
    "conf.set('spark.ui.port','4052')\n",
    "\n",
    "conf.set('spark.executor.heartbeatInterval','60s')\n",
    "conf.set(\"spark.driver.cores\",\"4\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\",\"-Xmx4g -Xms4g\")\n",
    "\n",
    "#conf.set(\"spark.shuffle.blockTransferService\", \"nio\"); \n",
    "conf.set(\"spark.files.overwrite\",\"true\");\n",
    "conf.set(\"spark.kryoserializer.buffer\", \"70\"); \n",
    "conf.set(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\");\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\");\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"); \n",
    "#conf.set(\"spark.broadcast.compress\", \"true\"); \n",
    "conf.set(\"spark.shuffle.compress\", \"true\"); \n",
    "conf.set(\"spark.shuffle.spill.compress\", \"true\");\n",
    "conf.set(\"spark.app.name\", Job_Name)\n",
    "#conf.set(\"spark.io.compression.codec\",\"org.apache.spark.io.LZ4CompressionCodec\");\n",
    "#conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\"); \n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "conf.set('spark.driver.memory','20g') ### change 2\n",
    "conf.set('spark.cores.max',max_cores) ### change 3\n",
    "conf.set('spark.sql.shuffle.partitions','400')\n",
    "#conf.set('spark.sql.crossJoin.enabled', 'true')\n",
    "# conf.set('es.nodes',es_nodes)\n",
    "# conf.set('es.port',es_port)\n",
    "# conf.set('es.nodes.wan.only','true')\n",
    "conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)\n",
    "\n",
    "#conf.set('spark.es.net.http.auth.user','Spark')\n",
    "#conf.set('spark.es.net.http.auth.pass','Jarkpet1Sap3')\n",
    "conf.set('spark.num.executors','4')\n",
    "\n",
    "# conf.set('spark.es.net.http.auth.user', es_user)\n",
    "# conf.set('spark.es.net.http.auth.pass', es_pwd)\n",
    "\n",
    "conf.set('spark.es.mapping.date.rich','false')\n",
    "spark = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Load Data into PySpark DataFrames\n",
    "# Prodcom Data Frame\n",
    "import json, pprint, requests\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2022-12-16 11:08:44.220225'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hash\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import DecimalType\n",
    "import psycopg2\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import pytz\n",
    "starttime = time.time()\n",
    "start_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prod_url = \"jdbc:postgresql://10.35.12.194:5432/gpadmin\"\n",
    "prod_host = '10.35.12.194'\n",
    "prod_port = '5432'\n",
    "prod_dbname = 'gpadmin'\n",
    "user_prod=\"gpspark\"\n",
    "pwd_prod=\"spark@456\"\n",
    "dbschema=\"public\"\n",
    "\n",
    "\n",
    "prod_gpdb_spark_options ={\n",
    "    \"url\": \"jdbc:postgresql://{host}:{port}/{dbname}\".format(host=prod_host,port=prod_port, dbname=prod_dbname),\n",
    "    \"user\": \"{user}\".format(user=user_prod),\n",
    "    \"password\": \"{password}\".format(password=pwd_prod)\n",
    "} \n",
    "\n",
    "# Define the GPDB-Python connection options for PROD \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_gpdb_jdbc(col_str,dbtable,col_name=None,time_filter=None,partitionColumn=\"row_num\"):\n",
    "    \"\"\"\n",
    "    This is used to read gpdb with filter for columns and can apply other filter(date,values).\n",
    "    Time filter contains startdate,enddate\n",
    "    \"\"\"\n",
    "    gscPythonOptions = {\n",
    "                        \"url\": prod_url,\n",
    "                        \"user\": user_prod,\n",
    "                        \"password\": pwd_prod,\n",
    "                        \"dbschema\": dbschema,\n",
    "                        \"dbtable\": dbtable,\n",
    "                        \"partitionColumn\":partitionColumn,\n",
    "                        \"partitions\": 8,\n",
    "                        \"server.port\":\"1150-1170\"}\n",
    "   \n",
    "    if time_filter:\n",
    "        data = sqlContext.read.format(\"greenplum\").options(**gscPythonOptions).load()\\\n",
    "                .selectExpr(col_str).drop_duplicates().filter(col(col_name).between(to_timestamp(lit(time_filter['start_date']),\n",
    "                                                                 format='yyyy-MM-dd'),\n",
    "                                                    to_timestamp(lit(time_filter['end_date']),\n",
    "                                                                 format='yyyy-MM-dd')))\n",
    "    else :\n",
    "        data = sqlContext.read.format(\"greenplum\").options(**gscPythonOptions).load()\\\n",
    "                .selectExpr(col_str).drop_duplicates()\n",
    "   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_latest_progress():\n",
    "    \n",
    "    try: \n",
    "        gscPythonOptions = {\n",
    "                 \"url\":prod_url ,\n",
    "                 \"user\":user_prod ,\n",
    "                 \"password\": pwd_prod,\n",
    "                 \"dbschema\":\"customermart\",\n",
    "                 \"dbtable\": \"progress\",#table change\n",
    "                 \"server.port\":\"1150-1170\"} \n",
    "        \n",
    "        # this query will fetch till what date we have inserted the records in target\n",
    "        last_run= sqlContext.read.format(\"greenplum\").options(**gscPythonOptions).load()\\\n",
    "            .select('table_name','source','to_datetime').filter(col('table_name')=='customer_activity')\\\n",
    "            .filter(col('source')=='activity1')\n",
    "        \n",
    "        Max_last_run =last_run.select(max('to_datetime')).first()[0]\n",
    "\n",
    "\n",
    "        # if no record is available in progress then it will raise value error, in this case default value will be picked from except clause\n",
    "        if Max_last_run is None:\n",
    "            raise ValueError(\"No records found!\")\n",
    "\n",
    "        #print(\"Record Found. Progress updated till {}\".format(Max_last_run))\n",
    "\n",
    "    except Exception as E: \n",
    "        Max_last_run = datetime.datetime(2021, 5, 14, 0, 0)  # this is the default start date when no record is present in progress for customer_demographics table\n",
    "        print(\"Executed_except\",repr(E))\n",
    "    return Max_last_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generic function to save the progress from last run into Progress table\n",
    "def update_progress(table_name,source,time_filter,records,start_time,starttime,status):\n",
    "    output_index = \"progress\"\n",
    "    schema = \"customermart\"\n",
    "\n",
    "\n",
    "    import sys\n",
    "    import time\n",
    "    try:\n",
    "        \n",
    "                \n",
    "#         df_progress_values = [{\"table_name\":table_name,\"source\":source,\"from_datetime\":time_filter['start_date'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#                         ,\"to_datetime\":time_filter['end_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),'records':records,\n",
    "#                          'start_time':start_time}]\n",
    "\n",
    "        df_progress= sqlContext.createDataFrame([(table_name,source,\n",
    "                                       time_filter['start_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                                         time_filter['end_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),records)]\n",
    "                                     ,['table_name', 'source','from_datetime','to_datetime','records'])\\\n",
    "        .withColumn('start_time',lit(start_time)).withColumn('end_time',current_timestamp())\\\n",
    "                    .withColumn('executed_in_mins',lit((time.time() - starttime)/60))\\\n",
    "                                .withColumn('status',lit(status)).withColumn('remarks',lit(None))\n",
    "        \n",
    "#         schema = StructType([StructField(\"table_name\", StringType(),True),StructField(\"source\", StringType(),True),\\\n",
    "#                      StructField(\"from_datetime\", StringType(),True),StructField(\"to_datetime\", StringType(),True),\\\n",
    "#                      StructField(\"records\", IntegerType(),True),StructField(\"start_time\", StringType(),True)])\n",
    "\n",
    "#         df_progress = sqlContext.createDataFrame(df_progress_values,schema)\n",
    "\n",
    "        \n",
    "        df_progress.write.format(\"greenplum\")\\\n",
    "            .option(\"dbtable\",output_index).option('dbschema',schema)\\\n",
    "            .option(\"server.port\",\"1150-1170\").option(\"url\",prod_url)\\\n",
    "            .option(\"user\", user_prod).option(\"password\",pwd_prod).mode('append').save()\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        x = e\n",
    "        print(x)\n",
    "    else:\n",
    "        x = 'success'\n",
    "        print(\"Updated Progress for {table_name} from {source} until {to_datetime}. {records} records processed in this run\".format(table_name=table_name, source=source,to_datetime=time_filter['end_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),records=records))\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Generic update function which takes the records currently in temp table (created with prefix 1 to original customercoe table)\n",
    "# The function would first try to insert the records in original table. It that fails then it will upsert the records\n",
    "# Since there is no direct upsert query an update and insert query with where clause is used\n",
    "# This funciton just runs the SQL queries in GPDB and does not use spark\n",
    "def update_activity(conn_to,table_name,source,gpdb_spark_options = prod_gpdb_spark_options,\n",
    "                  schema=\"customermart\"):\n",
    "    conn_to.rollback()\n",
    "    primary_key = ['source_system_name','source_system_customer_id','activity_type','activity_id','commit_timestamp']\n",
    "    update_fields = ['feature_value_1', 'feature_value_2','feature_value_3', 'feature_value_4', 'feature_string_1', 'feature_string_2',\n",
    "                 'feature_date_1', 'feature_date_2', 'feature_date_3']\n",
    "\n",
    "    df_1 = sqlContext.read.format(\"greenplum\").options(**gpdb_spark_options).option(\"dbschema\",schema).option(\"dbtable\",\"{}_1_staging\".format(table_name)).load()\n",
    "    out_columns = df_1.columns\n",
    "    out_columns = [col for col in out_columns if \"json\" not in col]\n",
    "\n",
    "    cur_to = conn_to.cursor()  \n",
    "\n",
    "    only_insert_query = \"\"\"insert into {schema}.{table_name} ({out_columns}) select {out_columns} from {schema}.{table_name}_1_staging \"\"\".format(schema=schema,table_name=table_name,out_columns = \",\".join(out_columns))\n",
    "    update_query = \"\"\"UPDATE {schema}.{table_name} orig\n",
    "                          SET\n",
    "                            {update_fields}\n",
    "                          FROM\n",
    "                            {schema}.{table_name}_1_staging temp\n",
    "                           WHERE \n",
    "                            {primary_key}\n",
    "                       \"\"\".format(schema=schema,table_name=table_name,primary_key= \" and \".join([ \"orig.{key} = temp.{key}\".format(key=key) for key in primary_key]),update_fields = \" , \".join([ \"{key} = temp.{key}\".format(key=key) for key in update_fields]))\n",
    "    insert_query = \"\"\" INSERT INTO {schema}.{table_name} ({out_columns})\n",
    "                           SELECT {out_columns}\n",
    "                           FROM\n",
    "                             {schema}.{table_name}_1_staging temp\n",
    "                           WHERE\n",
    "                             NOT EXISTS (\n",
    "                             SELECT 1 FROM {schema}.{table_name} orig WHERE \n",
    "                            {primary_key}\n",
    "                            )\n",
    "                        \"\"\".format(schema=schema,table_name=table_name,out_columns = \",\".join(out_columns),primary_key = \" and \".join([ \"orig.{key} = temp.{key}\".format(key=key) for key in primary_key]))\n",
    "\n",
    "    try:\n",
    "        print(\"Inside try segment\")\n",
    "        cur_to.execute(only_insert_query)\n",
    "        print(\"Executed Only insert query\")\n",
    "#         update_progress(table_name,source,time_filter,records)\n",
    "    except Exception as e:\n",
    "        print(\"Inside except segment\")\n",
    "        conn_to.rollback()\n",
    "        cur_to.execute(update_query)\n",
    "        print(\"Executed update query\")\n",
    "        cur_to.execute(insert_query)\n",
    "        print(\"Executed insert query\")\n",
    "#         update_progress(table_name,source,time_filter,records)\n",
    "    conn_to.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_date': datetime.datetime(2022, 11, 30, 0, 0), 'end_date': datetime.datetime(2022, 12, 14, 0, 0)}"
     ]
    }
   ],
   "source": [
    "latest_progress=get_latest_progress()\n",
    "# dates in string format\n",
    "str_d1 = latest_progress.strftime(\"%Y-%m-%d\")\n",
    "str_d2 = datetime.datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# convert string to date object\n",
    "d1 = datetime.datetime.strptime(str_d1, \"%Y-%m-%d\")\n",
    "d2 = datetime.datetime.strptime(str_d2, \"%Y-%m-%d\")\n",
    "\n",
    "# difference between dates in timedelta\n",
    "delta = d2 - d1\n",
    "day=delta.days-2 #currently T-2 change 2 to 1 for T-1\n",
    "\n",
    "time_filter={'start_date':latest_progress,'end_date':latest_progress+datetime.timedelta(days=day)}\n",
    "time_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_str=[\"txt_customer_id\", \"txt_policy_no_char\",\"num_reference_number\" , \"num_product_code\" , \"txt_business_type\" , \"dat_policy_issue_date\" ,'dat_policy_eff_todate','txt_policy_eff_tohour', \"tgt_commit_timestamp\"]\n",
    "\n",
    "gen_prop=load_gpdb_jdbc(col_str,\"policy_gc_gen_prop_information_tab\",\"TGT_COMMIT_TIMESTAMP\",time_filter).filter(col(\"txt_business_type\").isNotNull())\\\n",
    "                      .filter(col(\"txt_customer_id\").isNotNull())\\\n",
    "                      .filter(col(\"txt_policy_no_char\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_prop=gen_prop.filter(~gen_prop[\"txt_business_type\"].isin(['Used Vehicle','Renewal Business']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gen_prop.select('txt_policy_eff_tohour', date_format('txt_policy_eff_tohour', '0000-00-00 HH:mm:ss.SSS')).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_prop=gen_prop.withColumn('source_system_customer_id', col('txt_customer_id'))\\\n",
    ".withColumn('activity_id', col('txt_policy_no_char'))\\\n",
    ".withColumn('reference_number', col('num_reference_number'))\\\n",
    ".withColumn('feature_string_1', col('num_product_code'))\\\n",
    ".withColumn('feature_string_2', col('txt_business_type'))\\\n",
    ".withColumn('feature_date_2', col('dat_policy_issue_date'))\\\n",
    ".withColumn(\"feature_date_3\", concat_ws('',trim(col('dat_policy_eff_todate')[0:11]),lit(' '),trim(col('txt_policy_eff_tohour')),lit(':00.000')))\\\n",
    ".withColumn('commit_timestamp', col('tgt_commit_timestamp'))\\\n",
    ".drop('txt_customer_id','txt_policy_no_char','num_reference_number','num_product_code','txt_business_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#extract hours, minutes, seconds from time and one by one to todate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o251.load.\n",
      ": org.postgresql.util.PSQLException: ERROR: Canceling query because of high VMEM usage. Used: 16870MB, available 0MB, red zone: 27620MB (runaway_cleaner.c:202)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2675)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:355)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:490)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:408)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:329)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:315)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:291)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeQuery(PgStatement.java:243)\n",
      "\tat com.zaxxer.hikari.pool.ProxyStatement.executeQuery(ProxyStatement.java:111)\n",
      "\tat com.zaxxer.hikari.pool.HikariProxyStatement.executeQuery(HikariProxyStatement.java)\n",
      "\tat io.pivotal.greenplum.spark.jdbc.Jdbc$.resolveTable(Jdbc.scala:299)\n",
      "\tat io.pivotal.greenplum.spark.GreenplumRelationProvider.createRelation(GreenplumRelationProvider.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/sql/readwriter.py\", line 172, in load\n",
      "    return self._df(self._jreader.load())\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o251.load.\n",
      ": org.postgresql.util.PSQLException: ERROR: Canceling query because of high VMEM usage. Used: 16870MB, available 0MB, red zone: 27620MB (runaway_cleaner.c:202)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2675)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:355)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:490)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:408)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:329)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:315)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:291)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeQuery(PgStatement.java:243)\n",
      "\tat com.zaxxer.hikari.pool.ProxyStatement.executeQuery(ProxyStatement.java:111)\n",
      "\tat com.zaxxer.hikari.pool.HikariProxyStatement.executeQuery(HikariProxyStatement.java)\n",
      "\tat io.pivotal.greenplum.spark.jdbc.Jdbc$.resolveTable(Jdbc.scala:299)\n",
      "\tat io.pivotal.greenplum.spark.GreenplumRelationProvider.createRelation(GreenplumRelationProvider.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gscPythonOptions = {\"url\": \"jdbc:postgresql://10.35.12.194:5432/gpadmin\",\n",
    "                    \"user\": \"gpspark\",\n",
    "                    \"password\": \"spark@456\",\n",
    "                    \"dbschema\": \"datamarts\",\n",
    "                    \"dbtable\": \"sales_all_details\",\n",
    "                    \"partitionColumn\":\"row_num\",\n",
    "                    \"partitions\": 8,\n",
    "                    \"server.port\": '1150-1170'}\n",
    "\n",
    "sales = sqlContext.read.format(\"greenplum\").options(**gscPythonOptions).load()\\\n",
    "                    .selectExpr(\"sum_insured\", \"premiumamount\",\"total_premium\", \"crs_rcpt_date\" , \"proposal_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'sales' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'sales' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "sales1=sales.groupBy('proposal_no').agg(F.max(F.col('sum_insured')).alias('feature_value_1')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'sales' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'sales' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales2=sales.withColumn('feature_value_2', col('premiumamount'))\\\n",
    ".withColumn('feature_value_3', col('total_premium'))\\\n",
    ".withColumn('feature_date_1', col('crs_rcpt_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_str=[\"ld_rate\", \"reference_num\",\"tgt_commit_timestamp\", 'lddesc']\n",
    "cnfgtr=load_gpdb_jdbc(col_str,\"reference_gc_cnfgtr_policy_ld_dtls\",\"load_date\",time_filter)\n",
    "from pyspark.sql.functions import lower\n",
    "cnfgtr=cnfgtr.filter(lower(cnfgtr.lddesc).like('%loading%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnfgtr=cnfgtr.groupBy('reference_num').agg(F.max(F.col('ld_rate')).alias('ld_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'sales2' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'sales2' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1= gen_prop.join(sales2, gen_prop.reference_number == sales2.proposal_no, \"left_outer\")\\\n",
    ".drop(sales2.proposal_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_1' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_1' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy('source_system_customer_id','activity_id').orderBy(F.col('commit_timestamp').desc())\n",
    "dfTemp = df_1.withColumn('row',F.row_number().over(w))\n",
    "dfTemp = dfTemp.filter(F.col('row')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dfTemp' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dfTemp' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2= dfTemp.join(sales1, dfTemp.reference_number == sales1.proposal_no, \"left_outer\")\\\n",
    ".drop(sales2.proposal_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_2' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_2' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_main= df_2.join(cnfgtr, df_2.reference_number == cnfgtr.reference_num, \"left_outer\")\\\n",
    ".drop( cnfgtr.reference_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_main' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_main' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_main' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_main' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_main=df_main.withColumn('source_system_name', lit('GC'))\\\n",
    "                .withColumn('activity_type', lit('New Policy Addition'))\\\n",
    "                .withColumn('feature_value_4', col('ld_rate'))\\\n",
    "                .withColumn('feature_value_4',F.col('feature_value_4')*1.0)\\\n",
    "                .withColumn('feature_date_1',F.to_timestamp(F.col('feature_date_1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_main' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_main' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w1 = Window.partitionBy('source_system_customer_id','activity_id').orderBy(F.col('commit_timestamp').desc())\n",
    "df_main = df_main.withColumn('row_num',F.row_number().over(w1))\n",
    "df_main = df_main.filter('row_num == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_main' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_main' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_main=df_main.select(col('source_system_name').cast('string'),col('source_system_customer_id').cast('string'),col('activity_type').cast('string'), col('activity_id').cast('string'),\\\n",
    "                           col('feature_value_1').cast(DoubleType()), col('feature_value_2').cast(DoubleType()),\\\n",
    "                        col('feature_value_3').cast(DoubleType()),col('feature_value_4').cast(DecimalType(38,10)),\\\n",
    "                        col('feature_string_1').cast('string'),col('feature_string_2').cast('string'),col('feature_date_1').cast(DateType()),\\\n",
    "                        col('feature_date_2').cast(\"timestamp\"),col('feature_date_3').cast('timestamp'),col('commit_timestamp').cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_main.persist(pyspark.StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records=df_main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_index = \"customer_activity_1_staging\"\n",
    "schema = \"customermart\"\n",
    "\n",
    "import sys\n",
    "try:\n",
    "    \n",
    "    df_main.write.format(\"greenplum\")\\\n",
    "    .option(\"dbtable\",output_index).option('dbschema',schema)\\\n",
    "    .option(\"server.port\",\"1150-1170\").option(\"url\",prod_url)\\\n",
    "    .option(\"user\", user_prod).option(\"password\",pwd_prod).mode('overwrite').save()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    x = e\n",
    "    print(x)\n",
    "else:\n",
    "    x = 200 #success\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pg import DB\n",
    "# db = DB(dbname=\"gpadmin\", user='gpspark', passwd='spark@456', host='10.35.12.194', port= 5432)\n",
    "# stag_SI = db.query(\"select sum(feature_value_1) from customermart.customer_activity_1_staging\").getresult()\n",
    "# stag_prem=db.query(\"select sum(feature_value_2) from customermart.customer_activity_1_staging\").getresult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df=df_main.agg({'feature_value_1': 'sum','feature_value_2':'sum'}).withColumnRenamed('sum(feature_value_1)', 'value1')\\\n",
    "# .withColumnRenamed('sum(feature_value_2)', 'value2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value1=stag_SI[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value1=df.select(df.value1).collect()[0][0]\n",
    "# value2=df.select(df.value2).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name='customer_activity'\n",
    "source='activity1'\n",
    "conn_prod = psycopg2.connect(host=prod_host, port=prod_port, user=user_prod, password=pwd_prod, dbname=prod_dbname)\n",
    "conn_to=conn_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " update_activity(conn_to,table_name,source,prod_gpdb_spark_options, schema=\"customermart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscPythonOptions = {\n",
    "         \"url\": prod_url,\n",
    "         \"user\": user_prod,\n",
    "         \"password\": pwd_prod,\n",
    "         \"dbtable\": \"customermart.customer_activity\",\n",
    "         \"server.port\":\"1150-1170\"}\n",
    "\n",
    "\n",
    "activity1_df = sqlContext.read.format(\"jdbc\").options(**gscPythonOptions).load()\\\n",
    ".select('source_system_name', 'source_system_customer_id', 'activity_type', 'activity_id', 'feature_value_1', 'feature_value_2', 'feature_value_3', 'feature_value_4', 'feature_string_1', 'feature_string_2', 'feature_date_1', 'feature_date_2', 'feature_date_3', 'commit_timestamp')\\\n",
    ".filter(col('activity_type')=='New Policy Addition')\\\n",
    ".filter(col('commit_timestamp').between(to_timestamp(lit(time_filter['start_date']),\n",
    "                                                                 format='yyyy-MM-dd'),\n",
    "                                                    to_timestamp(lit(time_filter['end_date']),\n",
    "                                                                 format='yyyy-MM-dd'))).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stag=df_main.select(['source_system_name','source_system_customer_id','activity_type','activity_id','commit_timestamp','feature_value_1', 'feature_value_2','feature_value_3', 'feature_value_4', 'feature_string_1', 'feature_string_2',\n",
    "                 'feature_date_1', 'feature_date_2', 'feature_date_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity1_df1=activity1_df.select(['source_system_name','source_system_customer_id','activity_type','activity_id','commit_timestamp','feature_value_1', 'feature_value_2','feature_value_3', 'feature_value_4', 'feature_string_1', 'feature_string_2',\n",
    "                 'feature_date_1', 'feature_date_2', 'feature_date_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MissingData=df_stag.exceptAll(activity1_df1).withColumn('DataMisMatch',lit('Missing values at Target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MissingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconresult=[{\n",
    "#     \"SrcSI\":df.select(df.value1.cast('decimal(38,3)')).collect()[0][0],\n",
    "#     \"TargetSI\":stag_SI[0][0],\n",
    "#     \"SrcPrem\":df.select(df.value1.cast('decimal(38,3)')).collect()[0][0],\n",
    "#     \"TrgetPrem\":stag_prem[0][0],\n",
    "    \"TrgtMissMatchCount\":df_MissingData.count()\n",
    "}]\n",
    "\n",
    "from pyspark.sql import Row\n",
    "df_recon=sqlContext.createDataFrame(Row(**x) for x in reconresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon=df_recon.withColumn('Status',when((df_recon.TrgtMissMatchCount==0),'Success').otherwise('Failed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status=df_recon.select(df_recon.Status).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status='Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update_progress(table_name,source,time_filter,records,start_time,starttime,status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
